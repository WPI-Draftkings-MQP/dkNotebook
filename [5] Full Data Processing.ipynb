{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is used to generate the final data set for machine learning. It cleans and combines Header Data and Time series data into a features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Of the header data, these are the columns that are numeric values.\n",
    "scaleColumns = ['EntryFeeAmount', 'TotalPrizeAmount','MaxNumberPlayers', 'MaxEntriesPerUser', 'DistinctUsers', 'NumGames', 'DraftablePlayersInSet','PaidUsersInDraftGroup', 'TopPrize', 'MaxPayoutPosition', 'Duration']\n",
    "#Of the header data, these are the columns that are categorical.\n",
    "categoricalColumns = ['SportName', 'VariantName', 'Contest_Group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawDF = pd.read_csv('data/WorkingData.csv')\n",
    "rawDF = pd.merge(rawDF, pd.read_csv('data/Durations.csv').set_index('ContestId'), on='ContestId', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the columns not used in the final dataset:\n",
      "['ContestId', 'DraftGroupId', 'GameSet', 'ContestName', 'ContestStartDatetimeEST', 'ContestEndDatetimeEST', 'ContestPayoutDatetimeEST', 'Entries']\n"
     ]
    }
   ],
   "source": [
    "print(\"These are the columns not used in the final dataset:\")\n",
    "print(list(rawDF.drop(columns=scaleColumns).drop(columns=categoricalColumns).columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contests that do not fill to goal: 54481/630446\n"
     ]
    }
   ],
   "source": [
    "print(\"Contests that do not fill to goal:\", str(len(rawDF[~(rawDF['MaxNumberPlayers'] == rawDF['Entries'])])) + \"/\"+str (len(rawDF)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are synonymous, so I'm joining them\n",
    "rawDF = rawDF.replace('SOC', 'SOCC')\n",
    "rawDF = rawDF.replace('PGA', 'GOLF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedDF = rawDF.copy(deep=True).set_index('ContestId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Header Data Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "579223 contests hit 98%\n",
    "\n",
    "575965 contests hit 100%\n",
    "\n",
    "3258 contests change their success value by thresholding at 98%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Header data processing\n",
    "metaColumns = []\n",
    "successCol = []\n",
    "\n",
    "processedDF['Success'] = (processedDF['MaxNumberPlayers'] == processedDF['Entries'])\n",
    "successCol.append('Success')\n",
    "\n",
    "successDF = processedDF[['Success']]\n",
    "\n",
    "## Scale numeric data by max of each column\n",
    "for scol in scaleColumns:\n",
    "    processedDF[scol+\"_Scaled\"] = processedDF[scol]/processedDF[scol].max()\n",
    "    metaColumns.append(scol+\"_Scaled\")\n",
    "    \n",
    "## 1-hot encoding for categorical contests\n",
    "for scol in categoricalColumns:\n",
    "    for val in processedDF[scol].unique():\n",
    "        catStr = scol+\"_\"+val\n",
    "        processedDF[catStr] = (processedDF[scol]==val)\n",
    "        metaColumns.append(catStr)\n",
    "        \n",
    "metaDF = processedDF[metaColumns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "metaDF.to_csv('data/MetaData.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Pred Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Kalman Filter and WLS pred columns are all labeld \"Pred_*\". \n",
    "#Now they will be labeled \"Pred_KF_*\" and \"Pred_LR_*\"\n",
    "def renamePredColumns(df, prefix):\n",
    "    renameDict = {}\n",
    "    columns = []\n",
    "    for col in df.columns:\n",
    "        if(col[:4]=='Pred'):\n",
    "            colName = \"Pred_\"+prefix+\"_\"+col[4:]\n",
    "            renameDict[col] = colName\n",
    "            columns.append(colName)\n",
    "    df = df.rename(columns=renameDict)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Kalman Filter and WLS AB columns are all labeled \"A_*\"/\"B_*\". \n",
    "#Now they will be labeled \"A_KF_*\"/\"B_KF_*\" and \"A_LR_*\"/\"B_LR_*\"\n",
    "def renameABColumns(df, prefix):\n",
    "    renameDict = {}\n",
    "    columns = []\n",
    "    for col in df.columns:\n",
    "        colName = col\n",
    "        version = 'default'\n",
    "        if(col[-1:][0]=='A'):\n",
    "            version = col[:-1]\n",
    "        elif(col[0] == 'A'):\n",
    "            version = col[1:]\n",
    "        \n",
    "        if(not (version == 'default')):\n",
    "            newName = \"A_\"+prefix+\"_\"+version\n",
    "            renameDict[colName] = newName\n",
    "            renameDict[colName.replace('A', 'B')] = newName.replace('A', 'B')\n",
    "            columns.append(newName)\n",
    "            columns.append(newName.replace('A', 'B'))\n",
    "    df = df.rename(columns=renameDict)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getABDF(dfs):\n",
    "    for df in dfs:\n",
    "        print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limits columns in a DF to the larget value storable in np.float32. \n",
    "#This is to ensure that the machine learning algorithms can process the data\n",
    "MAX_FLOAT = np.finfo(np.float32).max\n",
    "def limitVal(df, ignore = [], N = MAX_FLOAT):\n",
    "    df = df.copy(deep=True)\n",
    "    for c in df.drop(columns=ignore).columns:\n",
    "        df[c] = df[c].where(df[c] <= N, N) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrDF = pd.read_csv('data/LR_Preds.csv').set_index('ContestId').drop(columns=['Duration'])\n",
    "lrDF = renamePredColumns(lrDF, 'LR')\n",
    "lrDF = renameABColumns(lrDF, 'LR')\n",
    "\n",
    "kfDF = pd.read_csv('data/KF_Preds.csv').set_index('ContestId').drop(columns=['Duration'])\n",
    "kfDF = renamePredColumns(kfDF, 'KF')\n",
    "kfDF = renameABColumns(kfDF, 'KF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "predDF = pd.merge(rawDF, kfDF, on='ContestId', how='left')\n",
    "predDF = pd.merge(predDF, lrDF, on='ContestId', how='left')\n",
    "predDF = predDF.set_index('ContestId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfColumns = []\n",
    "lrColumns = []\n",
    "for col in kfDF.columns:\n",
    "    if(col[:4]=='Pred'):\n",
    "        label = col+\"_Scaled\"\n",
    "        if(scaled):\n",
    "            predDF[label] = predDF[col]/100\n",
    "        else:\n",
    "            predDF[label] = predDF[col]/predDF[\"MaxNumberPlayers\"]\n",
    "        kfColumns.append(label)\n",
    "    else:\n",
    "        kfColumns.append(col)\n",
    "kfPredsDF = predDF[kfColumns]\n",
    "kfPredsDF = limitVal(kfPredsDF.dropna(), N=np.finfo(np.float32).max)\n",
    "        \n",
    "for col in lrDF.columns:\n",
    "    if(col[:4]=='Pred'):\n",
    "        label = col+\"_Scaled\"\n",
    "        if(scaled):\n",
    "            predDF[label] = predDF[col]/100\n",
    "        else:\n",
    "            predDF[label] = predDF[col]/predDF[\"MaxNumberPlayers\"]\n",
    "        lrColumns.append(label)\n",
    "    else:\n",
    "        lrColumns.append(col)\n",
    "        \n",
    "lrPredsDF = predDF[lrColumns]\n",
    "lrPredsDF = limitVal(lrPredsDF.dropna(), N=np.finfo(np.float32).max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfPredsDF.to_csv('data/KF_Results.csv')\n",
    "lrPredsDF.to_csv('data/LR_Results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "workedData = pd.merge(metaDF, kfPredsDF, on='ContestId', how='left')\n",
    "workedData = pd.merge(workedData, lrPredsDF, on='ContestId', how='left')\n",
    "workedData = pd.merge(successDF, workedData, on='ContestId', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "workedData.to_csv('data/WorkedData.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
